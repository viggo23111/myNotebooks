{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff65e05-4192-4d88-b3c6-f97a2ea830a4",
   "metadata": {},
   "source": [
    "# steps\n",
    "# Part 3\n",
    "1. TextBlob returns polarity and subjectivity of a sentence. Polarity lies between [-1,1], -1 defines a negative sentiment and 1 defines a positive sentiment. Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information\n",
    "\n",
    "2. Install both textblob for sentiment analysis and wordclouds (pip install textblob wordclouds) and download the vader lexicon (nltk.download('vader_lexicon'))\n",
    "3. Find the polarity and subjectivity of each text (Hint: TextBlob(text).sentiment)\n",
    "4. Is there a correlation between negativity and recession years?\n",
    "5. Create a word cloud for the cleaned up speeches of both Trump and Obama. What can be learned from the word clouds?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af57a2-cb83-4aa6-b6b8-8bff63de7fda",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "1. Get data from: \"https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents\"\n",
    "2. Using BeatifullSoup get all the speeches from 1900-2022\n",
    "3. Load all speech urls into a dictionary with year as key\n",
    "4. Loop through dictionary and save content of each speech in [year].txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a50754a3-712e-4a06-931f-a4ac96ba7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import re\n",
    "\n",
    "r = requests.get('https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents')\n",
    "soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "years=[]\n",
    "urls=[]\n",
    "\n",
    "year_reg= re.compile(r\"\\b(19|20)\\d{2}\\b\")\n",
    "speeches = soup.select(\"li\")[144:269]\n",
    "for s in speeches:\n",
    "    urls.append(\"https://en.wikisource.org\"+s.a['href'])\n",
    "    years.append(year_reg.search(s.text).group())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91873561-70d8-494b-8bd1-437a4033bcaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year_url_dict=dict(zip(years,urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "29687338-9cee-404e-9b39-07fbf5f188dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year_url in year_url_dict:\n",
    "    speechurl = requests.get(year_url_dict[year_url])\n",
    "    soup = bs4.BeautifulSoup(speechurl.text, 'html.parser')\n",
    "    ptags = soup.find(id=\"mw-content-text\").find_all('p')[1:-1]\n",
    "    f = open(\"data/\"+year_url+\".txt\", \"w\")\n",
    "    for p in ptags:\n",
    "        f.writelines(p.text)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d291e0-e2af-4aae-8ba7-6d473867ca76",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "1. Install nltk: pip install nltk\n",
    "2. From the data/gdp.csv file create a dataframe with year and GDP\n",
    "3. From the data/US presidents.csv file create a dataframe with year, president and party\n",
    "4. From the developed text files in part 1, create a dictionary with year:speech\n",
    "5. Clean text by change all to lowercase and remove '\\n'\n",
    "6. Get words from texts (from nltk.tokenize import word_tokenize). Clean text by removing stop words (from nltk.corpus import stopwords) and all non-alphabetic characters (including , and .)\n",
    "7. Use from nltk.stem import WordNetLemmatizer to lemmatize all texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5007a1e-b8a9-4bed-97da-07c52d083dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2022.3.2)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df_gdp\n",
    "df_pres"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
